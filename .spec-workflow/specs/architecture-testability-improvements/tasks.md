# Tasks Document

## Phase 1: Foundation

- [x] 1. Create Domain layer structure and interfaces
  - Files:
    - TeaLauncher.Avalonia/Domain/Interfaces/ICommandRegistry.cs
    - TeaLauncher.Avalonia/Domain/Interfaces/ICommandExecutor.cs
    - TeaLauncher.Avalonia/Domain/Interfaces/IAutoCompleter.cs
    - TeaLauncher.Avalonia/Domain/Interfaces/IConfigurationLoader.cs
    - TeaLauncher.Avalonia/Domain/Interfaces/IHotkeyManager.cs
    - TeaLauncher.Avalonia/Domain/Interfaces/IIMEController.cs
    - TeaLauncher.Avalonia/Domain/Models/Command.cs
  - Create Domain layer directory structure with all interface definitions
  - Extract Command model from CommandManager.cs to Domain/Models
  - Purpose: Establish clean architecture foundation with interface contracts
  - _Leverage: CommandLauncher/CommandManager.cs (for Command struct), TeaLauncher.Avalonia/Configuration/CommandConfig.cs_
  - _Requirements: 1.1, 2.1, 2.2, 4.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Software Architect specializing in clean architecture and interface design | Task: Create complete Domain layer structure following requirements 1.1, 2.1, 2.2, and 4.1. Create Domain/Interfaces/ directory with all six interfaces (ICommandRegistry with methods RegisterCommand, RemoveCommand, ClearCommands, HasCommand, GetAllCommands; ICommandExecutor with ExecuteAsync, GetExecution, GetArguments; IAutoCompleter with AutoCompleteWord, GetCandidates, UpdateWordList; IConfigurationLoader with LoadConfiguration, LoadConfigurationAsync; IHotkeyManager with RegisterHotkey, UnregisterHotkey, IsRegistered property; IIMEController with DisableIME, EnableIME). Extract Command struct from CommandManager.cs and convert to record in Domain/Models/Command.cs with properties Name, LinkTo, Description?, Arguments?. Use modern C# 12 features (record types, required keyword, nullable reference types). | Restrictions: Do not implement any logic, only interface definitions and models. Do not modify existing CommandManager.cs yet. Ensure all interfaces use Task for async operations. Do not add dependencies beyond System namespaces. | Leverage: Reference existing CommandManager.cs methods to design interface contracts. Reference CommandConfig.cs for model patterns. | Requirements: Requirement 1.1 (Clean Architecture - Domain layer contains interfaces), Requirement 2.1 (SOLID - Interface Segregation), Requirement 2.2 (SOLID - Dependency Inversion), Requirement 4.1 (Interface extraction for testability). | Success: All six interfaces created with XML documentation, Command record created with proper nullable annotations, compiles without errors, no implementation code in Domain layer, proper namespace (TeaLauncher.Avalonia.Domain.Interfaces and .Models). Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 2. Create Application layer structure
  - Files:
    - TeaLauncher.Avalonia/Application/Services/CommandRegistryService.cs
    - TeaLauncher.Avalonia/Application/Services/CommandExecutorService.cs
    - TeaLauncher.Avalonia/Application/Services/AutoCompleterService.cs
    - TeaLauncher.Avalonia/Application/Orchestration/ApplicationOrchestrator.cs
  - Create Application layer directory structure with empty service implementations
  - Purpose: Set up service layer skeleton implementing Domain interfaces
  - _Leverage: Domain/Interfaces/ (from task 1)_
  - _Requirements: 1.1, 2.1, 3.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: .NET Developer specializing in service layer architecture | Task: Create Application layer structure following requirements 1.1, 2.1, and 3.1. Create Application/Services/ directory with three service classes: CommandRegistryService implementing ICommandRegistry, CommandExecutorService implementing ICommandExecutor, AutoCompleterService implementing IAutoCompleter. Create Application/Orchestration/ directory with ApplicationOrchestrator class. Each service should have constructor with injected dependencies and throw NotImplementedException for all interface methods temporarily. Add XML documentation for each class describing its purpose. | Restrictions: Do not implement actual logic yet (will be done in later tasks). Do not add business logic. Only create class skeletons with proper interface implementation and constructor dependency injection. Use proper namespace TeaLauncher.Avalonia.Application.Services and .Orchestration. | Leverage: Use Domain interfaces created in task 1. Reference existing CommandManager.cs to understand which services need which dependencies (e.g., CommandRegistryService needs IAutoCompleter to update word list). | Requirements: Requirement 1.1 (Clean Architecture - Application layer for services), Requirement 2.1 (SOLID - Single Responsibility), Requirement 3.1 (Dependency Injection Container). | Success: All four service classes created with interface implementation, constructors with dependency injection parameters, NotImplementedException placeholders for methods, compiles without errors, proper namespaces. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 3. Set up dependency injection in Program.cs and App.axaml.cs
  - Files:
    - TeaLauncher.Avalonia/Program.cs
    - TeaLauncher.Avalonia/App.axaml.cs
    - TeaLauncher.Avalonia/ServiceConfiguration.cs (new)
    - TeaLauncher.Avalonia/TeaLauncher.Avalonia.csproj
  - Configure Microsoft.Extensions.DependencyInjection container
  - Register all services with appropriate lifetimes
  - Purpose: Enable dependency injection throughout application
  - _Leverage: Application/Services/ (from task 2), Domain/Interfaces/ (from task 1)_
  - _Requirements: 3.1, 3.2, 3.3_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in .NET dependency injection and IoC containers | Task: Set up complete DI infrastructure following requirements 3.1, 3.2, and 3.3. First, add NuGet package Microsoft.Extensions.DependencyInjection (version 8.0 or later) to TeaLauncher.Avalonia.csproj. Create new file ServiceConfiguration.cs with static class containing ConfigureServices extension method on IServiceCollection. Register services with lifetimes: Singleton for ICommandRegistry→CommandRegistryService, IAutoCompleter→AutoCompleterService, ICommandExecutor→CommandExecutorService, IConfigurationLoader→YamlConfigLoaderService, IHotkeyManager→WindowsHotkeyService, IIMEController→WindowsIMEControllerService; Transient for ApplicationOrchestrator and MainWindow. Modify Program.cs to create ServiceCollection, call ConfigureServices, and build ServiceProvider. Modify App.axaml.cs OnFrameworkInitializationCompleted to resolve MainWindow from ServiceProvider instead of direct instantiation. Store ServiceProvider in App class for access throughout application. | Restrictions: Do not modify service implementations yet. Use proper service lifetimes (Singleton for stateful/platform services, Transient for per-operation). Do not use service locator anti-pattern. Only constructor injection. | Leverage: Reference design document section "Dependency Injection Configuration" for exact registration pattern. Reference existing YamlConfigLoader, WindowsHotkey, WindowsIMEController that need to be registered as interfaces (they'll be refactored in later tasks). | Requirements: Requirement 3.1 (DI container with service registrations), Requirement 3.2 (appropriate service lifetimes), Requirement 3.3 (constructor injection). | Success: Microsoft.Extensions.DependencyInjection NuGet package added, ServiceConfiguration.cs created with all service registrations, Program.cs and App.axaml.cs modified to use DI container, application compiles (may not run yet due to NotImplementedException), MainWindow receives dependencies via constructor. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 4. Refactor existing services to implement interfaces
  - Files:
    - TeaLauncher.Avalonia/Infrastructure/Configuration/YamlConfigLoaderService.cs (rename and refactor from YamlConfigLoader.cs)
    - TeaLauncher.Avalonia/Infrastructure/Platform/WindowsHotkeyService.cs (rename and refactor from WindowsHotkey.cs)
    - TeaLauncher.Avalonia/Infrastructure/Platform/WindowsIMEControllerService.cs (rename and refactor from WindowsIMEController.cs)
  - Refactor existing infrastructure services to implement Domain interfaces
  - Purpose: Make existing platform-specific code injectable and testable
  - _Leverage: Configuration/YamlConfigLoader.cs, Platform/WindowsHotkey.cs, Platform/WindowsIMEController.cs, Domain/Interfaces/ (from task 1)_
  - _Requirements: 1.1, 4.1, 4.2_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: .NET Refactoring Specialist with expertise in interface extraction and clean architecture | Task: Refactor existing infrastructure services following requirements 1.1, 4.1, and 4.2. Create Infrastructure/ directory structure with Configuration/ and Platform/ subdirectories. Rename and move Configuration/YamlConfigLoader.cs to Infrastructure/Configuration/YamlConfigLoaderService.cs and implement IConfigurationLoader interface (add async version LoadConfigurationAsync wrapping synchronous LoadConfiguration). Rename and move Platform/WindowsHotkey.cs to Infrastructure/Platform/WindowsHotkeyService.cs and implement IHotkeyManager interface (RegisterHotkey method wrapping existing registration logic, UnregisterHotkey wrapping Dispose, IsRegistered property). Rename and move Platform/WindowsIMEController.cs to Infrastructure/Platform/WindowsIMEControllerService.cs and implement IIMEController interface. Preserve all existing implementation logic, only add interface implementation and rename classes. Update all usages in existing code to use new names. Update ServiceConfiguration.cs registrations to use new class names. | Restrictions: Do not change business logic or implementation. Only add interface implementation and rename. Preserve all existing P/Invoke code, exception handling, and behavior. Do not modify CommandManager.cs yet. Update namespaces to TeaLauncher.Avalonia.Infrastructure.Configuration and .Platform. | Leverage: Keep exact same implementation logic from existing files. Reference Domain interfaces for method signatures. Reference design document section "Infrastructure Layer Components" for refactoring patterns. | Requirements: Requirement 1.1 (Clean Architecture - Infrastructure layer), Requirement 4.1 (Interface extraction), Requirement 4.2 (SOLID - Liskov Substitution). | Success: All three services moved to Infrastructure/ structure, renamed with "Service" suffix, implement corresponding interfaces, preserve existing logic exactly, compile without errors, ServiceConfiguration updated, old files deleted, namespaces updated. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 2: Service Refactoring

- [x] 5. Refactor CommandManager into CommandRegistryService
  - File: TeaLauncher.Avalonia/Application/Services/CommandRegistryService.cs
  - Implement command registration and management logic
  - Purpose: Extract command registry responsibility from CommandManager
  - _Leverage: CommandLauncher/CommandManager.cs (RegisterCommand, RemoveCommand, ClearCommands, HasCommand methods), Domain/Models/Command.cs, Domain/Interfaces/IAutoCompleter.cs_
  - _Requirements: 2.1, 2.2, 7.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: C# Developer with expertise in SOLID principles and refactoring | Task: Implement CommandRegistryService following requirements 2.1 (Single Responsibility), 2.2 (Liskov Substitution), and 7.1 (Unit testing). Extract registration logic from CommandManager.RegisterCommand, RemoveCommand, ClearCommands, and HasCommand methods. Maintain private List<Command> m_Commands field (use _commands with underscore prefix following modern C# conventions). Constructor should inject IAutoCompleter dependency. When commands are registered/removed, update the IAutoCompleter word list by calling UpdateWordList with current command names. Implement GetAllCommands returning IReadOnlyList<Command>. Use case-insensitive command lookup (preserve existing behavior from CommandManager). Handle duplicate command registration (replace existing command with same name). | Restrictions: Do not implement command execution logic (that's CommandExecutorService). Do not add special command handling. Use dependency injection for IAutoCompleter, not direct instantiation. Follow existing behavior from CommandManager exactly. Use modern C# conventions (_camelCase for private fields, not m_ prefix). | Leverage: Copy logic from CommandManager.RegisterCommand, RemoveCommand, ClearCommands, HasCommand, and m_Commands field. Reference Domain models and interfaces created in task 1. Reference design document section "CommandRegistryService". | Requirements: Requirement 2.1 (SRP - single responsibility for registry), Requirement 2.2 (LSP - proper interface implementation), Requirement 7.1 (testable design). | Success: CommandRegistryService fully implements ICommandRegistry, maintains command list, synchronizes with auto-completer, handles duplicates, case-insensitive lookup works, compiles without errors, logic matches original CommandManager behavior. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 6. Refactor CommandManager into CommandExecutorService
  - File: TeaLauncher.Avalonia/Application/Services/CommandExecutorService.cs
  - Implement command execution logic
  - Purpose: Extract command execution responsibility from CommandManager
  - _Leverage: CommandLauncher/CommandManager.cs (Run, IsPath, Split, GetExecution, GetArguments methods), Domain/Interfaces/ICommandRegistry.cs_
  - _Requirements: 2.1, 2.2, 7.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in command execution and process management | Task: Implement CommandExecutorService following requirements 2.1, 2.2, and 7.1. Extract execution logic from CommandManager.Run method to ExecuteAsync (make async with Task return). Constructor should inject ICommandRegistry dependency to look up commands. Implement private helper methods: IsPath (detect URLs with http://, https://, ftp:// and file paths with drive letters), Split (parse command line into command name and arguments respecting quotes), IsSpecialCommand (check for !, ?, @ prefixes). ExecuteAsync should: lookup command from registry, split input into command + args, detect if input is direct path/URL (execute directly), handle special commands (throw NotImplementedException for now with "Special commands handled by orchestrator" message), execute regular commands via Process.Start, handle Win32Exception with clear error messages. Implement GetExecution (return LinkTo for command) and GetArguments (return command arguments). Preserve all error handling and validation from original Run method. | Restrictions: Do not implement special command logic (delegate to ApplicationOrchestrator). Use ICommandRegistry for lookups, not direct command list access. Make ExecuteAsync truly async (use Task.Run if needed for Process.Start). Use modern async/await patterns. Do not show dialogs directly (throw exceptions to be handled by caller). | Leverage: Copy logic from CommandManager.Run, IsPath, Split, GetExecution, GetArguments methods. Reference ICommandRegistry interface for command lookup. Reference design document section "CommandExecutorService". Preserve exact path detection heuristics and command parsing logic. | Requirements: Requirement 2.1 (SRP - single responsibility for execution), Requirement 2.2 (LSP - proper interface implementation), Requirement 7.1 (testable design with injected dependencies). | Success: CommandExecutorService fully implements ICommandExecutor, executes commands via Process.Start, handles paths and URLs correctly, parses arguments with quote handling, delegates special commands, proper async implementation, error handling preserved, compiles without errors. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 7. Refactor AutoCompleteMachine into AutoCompleterService
  - File: TeaLauncher.Avalonia/Application/Services/AutoCompleterService.cs
  - Implement auto-completion logic
  - Purpose: Extract auto-completion responsibility from AutoCompleteMachine
  - _Leverage: CommandLauncher/AutoCompleteMachine.cs (entire class)_
  - _Requirements: 2.1, 7.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Algorithm Developer with expertise in string matching and data structures | Task: Implement AutoCompleterService following requirements 2.1 and 7.1. Extract entire logic from AutoCompleteMachine.cs - this is a pure logic component with no dependencies. Maintain private List<string> _words field for word list storage. Implement AutoCompleteWord (find longest common prefix matching input), GetCandidates (return all words starting with prefix, case-insensitive), UpdateWordList (replace entire word list with new list). Preserve exact algorithm from AutoCompleteMachine: prefix matching using StartsWith, longest common prefix calculation, case-insensitive comparison using ToLower(). No constructor dependencies needed (stateless logic, only maintains word list). | Restrictions: Do not change auto-completion algorithm logic. Do not add external dependencies. Keep pure string matching implementation. Use modern C# conventions (_words not m_Words). Do not modify AutoCompleteMachine.cs file (it's linked from CommandLauncher project, keep for backward compatibility). | Leverage: Copy entire implementation from AutoCompleteMachine.cs exactly. Reference IAutoCompleter interface from task 1. Reference design document section "AutoCompleterService". Preserve prefix matching algorithm and longest common prefix logic. | Requirements: Requirement 2.1 (SRP - single responsibility for auto-completion), Requirement 7.1 (testable design without UI dependencies). | Success: AutoCompleterService fully implements IAutoCompleter, auto-completion algorithm works identically to AutoCompleteMachine, prefix matching and candidate generation correct, case-insensitive comparison works, UpdateWordList replaces word list, compiles without errors, unit testable. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 8. Create ApplicationOrchestrator
  - File: TeaLauncher.Avalonia/Application/Orchestration/ApplicationOrchestrator.cs
  - Implement application workflow coordination
  - Purpose: Orchestrate initialization, configuration reload, and special commands
  - _Leverage: CommandLauncher/CommandManager.cs (special command handling), Application/Services/ (from tasks 5-7), Infrastructure/ (from task 4)_
  - _Requirements: 1.1, 2.1, 3.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Software Architect specializing in application orchestration and workflow coordination | Task: Implement ApplicationOrchestrator following requirements 1.1, 2.1, and 3.1. Constructor should inject: IConfigurationLoader, ICommandRegistry, ICommandExecutor, IHotkeyManager. Implement methods: InitializeAsync (load config from path, register all commands, setup hotkey with callback), ReloadConfigurationAsync (clear commands, reload config, re-register commands), HandleSpecialCommand (switch on command: "!reload"→call ReloadConfigurationAsync, "!version"→return version info string, "!exit"→throw ExitApplicationException custom exception), GetVersion (return assembly version from AssemblyInfo). Special command detection: commands starting with ! are special. Store config file path as private field for reload. Hotkey callback should be passed as Action parameter to InitializeAsync. Use async/await properly throughout. Handle exceptions from config loading with clear error messages. | Restrictions: Do not implement UI dialog display logic (caller will handle displaying messages). Do not directly interact with Process.Start or platform APIs (use injected services). Do not maintain command list state (delegate to ICommandRegistry). Create custom exception ExitApplicationException for !exit command. Use modern async patterns throughout. | Leverage: Reference CommandManager.RunSpecialCommand, RequestReinitialize, RequestShowVersionInfo, RequestExitApplication methods for special command logic. Reference IConfigurationLoader, ICommandRegistry, IHotkeyManager interfaces. Reference design document section "ApplicationOrchestrator". Extract special command handling from CommandManager (!reload, !version, !exit). | Requirements: Requirement 1.1 (Clean Architecture - Application layer orchestration), Requirement 2.1 (SRP - orchestration responsibility), Requirement 3.1 (DI - injected dependencies). | Success: ApplicationOrchestrator implements all orchestration methods, InitializeAsync loads config and registers commands, ReloadConfigurationAsync works correctly, HandleSpecialCommand processes all special commands (!reload, !version, !exit), proper exception handling, async/await used correctly, compiles without errors. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 3: Testing Infrastructure

- [ ] 9. Set up unit test structure with NSubstitute
  - Files:
    - TeaLauncher.Avalonia.Tests/TeaLauncher.Avalonia.Tests.csproj
    - TeaLauncher.Avalonia.Tests/GlobalUsings.cs
  - Add NSubstitute NuGet package and configure test project
  - Purpose: Prepare testing infrastructure with mocking framework
  - _Leverage: Existing TeaLauncher.Avalonia.Tests project structure_
  - _Requirements: 7.1, 7.2, 7.3_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Engineer with expertise in .NET testing frameworks and NUnit | Task: Configure unit test infrastructure following requirements 7.1, 7.2, and 7.3. Add NuGet packages to TeaLauncher.Avalonia.Tests.csproj: NSubstitute (version 5.1.0 or later), FluentAssertions (version 6.12.0 or later) for better assertions. Update GlobalUsings.cs to include: using NUnit.Framework; using NSubstitute; using FluentAssertions; using TeaLauncher.Avalonia.Domain.Interfaces; using TeaLauncher.Avalonia.Domain.Models; using TeaLauncher.Avalonia.Application.Services. Create directory structure: Application/Services/, Application/Orchestration/, Infrastructure/Configuration/, Infrastructure/Platform/, Utilities/. Create TestBase.cs in project root with common test setup/teardown methods (SetUp, TearDown attributes). | Restrictions: Do not write actual tests yet (next tasks). Only set up infrastructure. Use NSubstitute not Moq (design decision for clarity). Add FluentAssertions for readable test assertions. Do not modify existing test files yet. Ensure test project targets net8.0-windows same as main project. | Leverage: Reference existing TeaLauncher.Avalonia.Tests.csproj for project structure. Reference design document section "Unit Testing" for framework choices. Use existing NUnit 4.x already in project. | Requirements: Requirement 7.1 (comprehensive unit testing), Requirement 7.2 (mocking framework), Requirement 7.3 (test patterns). | Success: NSubstitute and FluentAssertions packages added to test project, GlobalUsings.cs updated with all necessary usings, directory structure created mirroring main project, TestBase.cs created with common setup, compiles without errors, ready for test writing. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 10. Create test utilities (TestServiceProvider, MockFactory)
  - Files:
    - TeaLauncher.Avalonia.Tests/Utilities/TestServiceProvider.cs
    - TeaLauncher.Avalonia.Tests/Utilities/MockFactory.cs
    - TeaLauncher.Avalonia.Tests/Utilities/TestFixtures.cs
  - Create helper utilities for test setup and mock creation
  - Purpose: Provide reusable test infrastructure for DI and mocking
  - _Leverage: ServiceConfiguration.cs, Domain/Interfaces/_
  - _Requirements: 7.1, 7.2, 7.3_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Infrastructure Developer with expertise in test utilities and dependency injection | Task: Create test utility classes following requirements 7.1, 7.2, and 7.3. Create TestServiceProvider.cs with static methods: CreateWithMocks() returning IServiceProvider with all interfaces registered as NSubstitute mocks, CreateWithRealServices() returning IServiceProvider with real implementations, CreateCustom(Action<IServiceCollection> configure) allowing custom registration. Create MockFactory.cs with static methods for creating configured mocks: CreateMockCommandRegistry(params Command[] commands), CreateMockCommandExecutor(), CreateMockAutoCompleter(params string[] words), CreateMockConfigurationLoader(CommandsConfig config), etc. Create TestFixtures.cs with sample test data: SampleCommands (List<Command> with common test commands like "google", "docs", "github"), SampleYamlConfig (string with valid YAML), InvalidYamlConfig (string with syntax errors), ComplexCommandLine (string with quotes and arguments). | Restrictions: Do not create actual test cases yet. Only utility helpers. Use NSubstitute Substitute.For<T>() for mock creation. Ensure TestServiceProvider properly disposes ServiceProvider. Make all utilities static for easy access. | Leverage: Reference ServiceConfiguration.cs for service registration pattern. Reference Domain interfaces for mock creation. Reference design document section "Test Patterns" for utility design. Use Microsoft.Extensions.DependencyInjection for test DI container. | Requirements: Requirement 7.1 (comprehensive unit testing infrastructure), Requirement 7.2 (mocking framework usage), Requirement 7.3 (AAA pattern support). | Success: TestServiceProvider.cs created with DI container helpers, MockFactory.cs created with preconfigured mocks, TestFixtures.cs created with reusable test data, all utilities are static and easy to use, compiles without errors, ready for use in test cases. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 11. Write unit tests for all services (≥ 90% coverage)
  - Files:
    - TeaLauncher.Avalonia.Tests/Application/Services/CommandRegistryServiceTests.cs
    - TeaLauncher.Avalonia.Tests/Application/Services/CommandExecutorServiceTests.cs
    - TeaLauncher.Avalonia.Tests/Application/Services/AutoCompleterServiceTests.cs
    - TeaLauncher.Avalonia.Tests/Application/Orchestration/ApplicationOrchestratorTests.cs
    - TeaLauncher.Avalonia.Tests/Infrastructure/Configuration/YamlConfigLoaderServiceTests.cs
  - Write comprehensive unit tests following AAA pattern
  - Purpose: Achieve ≥ 90% code coverage for Domain and Application layers
  - _Leverage: Utilities/TestServiceProvider.cs, Utilities/MockFactory.cs, Utilities/TestFixtures.cs (from task 10)_
  - _Requirements: 7.1, 7.2, 7.3, 7.4_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Senior QA Engineer with expertise in unit testing, code coverage, and test-driven development | Task: Write comprehensive unit tests following requirements 7.1-7.4 achieving ≥90% code coverage. For each service, create test class with [TestFixture] attribute. Use AAA (Arrange-Act-Assert) pattern for all tests. Test naming: [MethodName]_[Scenario]_[ExpectedResult] (e.g., RegisterCommand_WithValidCommand_ShouldAddToRegistry). CommandRegistryServiceTests: test RegisterCommand (success, duplicate handling, null command), RemoveCommand (existing, non-existing), ClearCommands, HasCommand (case-insensitive), GetAllCommands, auto-completer synchronization. CommandExecutorServiceTests: test ExecuteAsync (regular command, direct URL, file path, special command delegation, missing command, invalid command), GetExecution, GetArguments, path detection, argument parsing with quotes. AutoCompleterServiceTests: test AutoCompleteWord (exact match, partial match, no match, empty input), GetCandidates (prefix matching, case-insensitive, empty results), UpdateWordList. ApplicationOrchestratorTests: test InitializeAsync (success, config load failure, hotkey registration), ReloadConfigurationAsync, HandleSpecialCommand (!reload, !version, !exit), error handling. YamlConfigLoaderServiceTests: test LoadConfiguration (valid YAML, invalid syntax, missing file, malformed structure), async version. Use NSubstitute mocks from MockFactory. Use FluentAssertions for assertions (.Should().Be(), .Should().NotBeNull(), etc.). Test both success and failure paths. Mock all dependencies. | Restrictions: Do not test UI components yet. Do not test Infrastructure/Platform services (WindowsHotkey, IME - will be mocked in integration tests). Focus on Application and Domain logic. Achieve ≥90% line coverage for tested services. Use async test methods where appropriate ([Test] public async Task). | Leverage: Use TestServiceProvider and MockFactory from task 10. Use TestFixtures for sample data. Reference design document section "Unit Testing" for test patterns and examples. Reference existing tests in Configuration/ and Platform/ directories for NUnit patterns. | Requirements: Requirement 7.1 (comprehensive unit tests), Requirement 7.2 (mocking framework), Requirement 7.3 (AAA pattern, clear naming), Requirement 7.4 (90% coverage target). | Success: All five test classes created, tests use AAA pattern, test names are descriptive, NSubstitute mocks used correctly, FluentAssertions used for readable assertions, ≥90% code coverage achieved for Application layer services, all tests pass, tests run in under 5 seconds. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 12. Write integration tests for critical flows
  - Files:
    - TeaLauncher.Avalonia.Tests/Integration/CommandWorkflowTests.cs
    - TeaLauncher.Avalonia.Tests/Integration/ConfigurationIntegrationTests.cs
    - TeaLauncher.Avalonia.Tests/Integration/Fixtures/test-commands.yaml
    - TeaLauncher.Avalonia.Tests/Integration/Fixtures/invalid-commands.yaml
  - Write integration tests using real DI container and real services
  - Purpose: Verify services work correctly together with real implementations
  - _Leverage: ServiceConfiguration.cs, Utilities/TestServiceProvider.cs, all implemented services_
  - _Requirements: 8.1, 8.2, 8.3, 8.4_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in end-to-end service testing | Task: Write integration tests following requirements 8.1-8.4. Create test fixtures: test-commands.yaml (valid YAML with 5 test commands), invalid-commands.yaml (YAML with syntax error at line 3). CommandWorkflowTests: test LoadConfig_RegisterCommands_ExecuteCommand_ShouldSucceed (full workflow: load YAML → register in registry → execute via executor → verify success), test LoadConfig_WithInvalidYaml_ShouldThrowException (verify error message contains line number), test Configuration_AutoCompleter_Integration (load config → verify auto-completer has command names → test completion). ConfigurationIntegrationTests: test LoadAndReload_ShouldUpdateCommands (initialize → modify YAML → reload → verify new commands), test MissingFile_ShouldProvideHelpfulError, test HotkeyRegistration_Integration (mock hotkey on Linux, verify callback wired correctly). Use real ServiceProvider with real service implementations (not mocks) from TestServiceProvider.CreateWithRealServices(). Use [SetUp] to create service provider, [TearDown] to dispose. Use temporary test files created/deleted during test. Mock only platform-specific services (IHotkeyManager, IIMEController) for Linux compatibility. Test real file I/O with test YAML files. Verify integration points between services. | Restrictions: Do not mock service implementations (use real services). Only mock platform-specific APIs (hotkey, IME). Tests must run on Linux without Windows APIs. Use temporary directories for test files (Path.GetTempPath()). Clean up test files in [TearDown]. Tests should complete in under 15 seconds total. Do not test UI components. | Leverage: Use TestServiceProvider.CreateWithRealServices() modified to mock platform services. Use ServiceConfiguration for real service registration. Reference design document section "Integration Testing" for test scenarios and patterns. Create test YAML files in Fixtures/ directory. | Requirements: Requirement 8.1 (integration test scenarios), Requirement 8.2 (real DI container), Requirement 8.3 (real file I/O), Requirement 8.4 (15 second completion). | Success: Both integration test classes created, test fixtures (YAML files) created, tests use real services with real DI container, platform services mocked for Linux compatibility, tests verify end-to-end workflows, file I/O tested with real files, temporary files cleaned up, tests complete in <15 seconds, all tests pass. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 4: Quality Gates

- [ ] 13. Install and configure Husky.Net
  - Files:
    - .husky/pre-commit
    - .husky/task-runner.json
    - TeaLauncher.Avalonia.sln (add Husky.Net)
  - Install Husky.Net and set up Git hooks infrastructure
  - Purpose: Enable pre-commit verification hooks
  - _Leverage: Git repository, existing .gitignore_
  - _Requirements: 5.1, 5.2_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in Git hooks and CI/CD automation | Task: Install and configure Husky.Net following requirements 5.1 and 5.2. Install Husky.Net as dotnet tool: run 'dotnet new tool-manifest' if not exists, then 'dotnet tool install Husky'. Initialize Husky: run 'dotnet husky install'. This creates .husky/ directory with pre-commit hook script and task-runner.json. Configure task-runner.json with task groups: "pre-commit" group containing tasks: "build" (dotnet build --configuration Release --no-restore), "test" (dotnet test --no-build --verbosity minimal), "format-check" (dotnet format --verify-no-changes). Add .husky/ directory to Git (not .gitignore). Update .gitignore to exclude .husky/_/ (Husky internal files). Test pre-commit hook by making a test commit (should run build and test). Document in comments how to bypass hooks with --no-verify if needed. | Restrictions: Do not add coverage verification yet (task 14). Do not add metrics checking yet (task 14). Only basic build and test in this task. Ensure Husky.Net uses task-runner.json not shell scripts (for cross-platform compatibility). Use --no-restore and --no-build flags where appropriate to speed up hooks. | Leverage: Reference Husky.Net documentation at alirezanet.github.io/Husky.Net/. Reference design document section "Husky.Net Configuration" for exact task-runner.json structure. Ensure hooks work on both Linux and Windows. | Requirements: Requirement 5.1 (pre-commit hooks automatically run), Requirement 5.2 (file and method metrics checked before commit). | Success: Husky.Net installed as dotnet tool, .husky/ directory created, task-runner.json configured with build and test tasks, pre-commit hook runs automatically on git commit, tasks complete in under 30 seconds, hook can be bypassed with --no-verify, documented in code comments. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 14. Create MetricsChecker tool
  - Files:
    - tools/MetricsChecker/MetricsChecker.csproj
    - tools/MetricsChecker/Program.cs
    - tools/MetricsChecker/CodeMetricsAnalyzer.cs
    - tools/MetricsChecker/MetricsReport.cs
  - Create custom tool using Roslyn to analyze code metrics
  - Purpose: Verify file length, method length, and cyclomatic complexity thresholds
  - _Leverage: Microsoft.CodeAnalysis.CSharp NuGet package (Roslyn)_
  - _Requirements: 5.2, 5.3_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Tools Developer with expertise in Roslyn code analysis and static analysis tools | Task: Create MetricsChecker console application following requirements 5.2 and 5.3. Create new .NET 8 console project in tools/MetricsChecker/ directory. Add NuGet package Microsoft.CodeAnalysis.CSharp.Workspaces (version 4.8 or later). Program.cs should: accept project path as argument, load project using MSBuildWorkspace, iterate all C# documents, analyze each file. CodeMetricsAnalyzer.cs should implement: AnalyzeFile(SyntaxTree) returning MetricsResult with violations. Check metrics: file line count ≤500, method line count ≤50 (excluding blank lines and comments), cyclomatic complexity ≤15 per method (use CSharpSyntaxWalker to count decision points: if, while, for, foreach, case, catch, &&, ||, ??). MetricsReport.cs should format violations as readable text with file path, line number, metric name, value, threshold. Exit code 0 if no violations, 1 if violations found. Support --exclude flag to skip directories (e.g., obj, bin, .git). Output violations to stdout in format: "VIOLATION: File.cs:123 - Method 'Foo' complexity 18 (max: 15)". | Restrictions: Do not use external metrics tools (SonarQube, CodeMetrics.exe). Use only Roslyn APIs for analysis. Keep tool simple and fast (complete in <5 seconds for TeaLauncher project). Count only meaningful lines (exclude blank lines and comments when counting method lines). Use accurate cyclomatic complexity calculation (decision points + 1). | Leverage: Reference Microsoft.CodeAnalysis documentation for Roslyn usage. Reference design document section "MetricsChecker Tool Design" for implementation pattern. Use CSharpSyntaxWalker for syntax tree traversal. Calculate complexity by counting: IfStatement, WhileStatement, ForStatement, ForEachStatement, CaseSwitchLabel, CatchClause, BinaryExpression with && or ||, CoalesceExpression. | Requirements: Requirement 5.2 (code metrics verification thresholds), Requirement 5.3 (clear error messages on violation). | Success: MetricsChecker.csproj created targeting net8.0, Roslyn packages added, CodeMetricsAnalyzer implements metric checking (file ≤500 lines, method ≤50 lines, complexity ≤15), MetricsReport formats violations clearly, tool runs in <5 seconds, exit code indicates pass/fail, compiles and runs successfully. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 15. Configure pre-commit hooks with coverage and metrics
  - Files:
    - .husky/task-runner.json (update)
    - .husky/pre-commit (update)
    - scripts/check-coverage.sh
    - scripts/check-metrics.sh
  - Add coverage verification and metrics checking to pre-commit hooks
  - Purpose: Enforce quality gates before every commit
  - _Leverage: tools/MetricsChecker (from task 14), dotnet test coverage_
  - _Requirements: 5.1, 5.2, 5.3, 6.1, 6.2_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: CI/CD Engineer with expertise in test automation and quality gates | Task: Configure complete pre-commit verification following requirements 5.1-5.3 and 6.1-6.2. Update .husky/task-runner.json to add tasks to "pre-commit" group: "coverage" task running 'dotnet test --no-build --collect:"XPlat Code Coverage" --results-directory ./coverage', "metrics" task running 'dotnet run --project tools/MetricsChecker/MetricsChecker.csproj -- TeaLauncher.Avalonia.csproj'. Create scripts/check-coverage.sh (bash script): parse Cobertura XML from coverage/ directory, extract line coverage percentage, compare to threshold (80%), exit 1 if below. Create scripts/check-metrics.sh (bash script): wrapper for MetricsChecker that handles cross-platform path differences. Update task-runner.json to add "coverage-check" and "metrics-check" tasks running respective scripts. Ensure task execution order: build → test → coverage → coverage-check → metrics → metrics-check → format-check. Add timeout to each task (build: 60s, test: 30s, coverage: 30s, metrics: 10s) to prevent hanging. | Restrictions: Coverage checking must parse actual Cobertura XML (not rely on dotnet test exit code). Metrics checking must use MetricsChecker tool output. Scripts must work on both Linux and Windows (use bash for cross-platform). Do not fail on test failures in coverage task (test task already checks). Only fail on coverage threshold violation. Ensure hooks complete in ≤30 seconds total. | Leverage: Use Coverlet XPlat Code Coverage built into dotnet test. Reference MetricsChecker tool from task 14. Parse coverage/*/coverage.cobertura.xml using grep/sed or xmllint. Reference design document section "Coverage Validation" for coverage checking approach. Use dotnet test --collect:"XPlat Code Coverage" for coverage generation. | Requirements: Requirement 5.1 (pre-commit automatic execution), Requirement 5.2 (metrics thresholds), Requirement 5.3 (clear failure messages), Requirement 6.1 (coverage thresholds 80%), Requirement 6.2 (coverage enforcement on commit). | Success: task-runner.json updated with coverage and metrics tasks, check-coverage.sh script created and works correctly, check-metrics.sh script created, scripts executable (chmod +x), pre-commit hook runs all tasks in order, coverage threshold enforced (80%), metrics thresholds enforced (500/50/15), clear error messages on violations, complete in ≤30 seconds, commits blocked on violations. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 16. Set up CI/CD quality gates
  - Files:
    - .github/workflows/ci.yml (or equivalent for your CI system)
    - .github/workflows/pr-validation.yml
  - Configure CI/CD pipeline with same quality checks as pre-commit hooks
  - Purpose: Ensure quality gates enforced in CI/CD, not just locally
  - _Leverage: .husky/task-runner.json tasks, scripts/check-coverage.sh, tools/MetricsChecker_
  - _Requirements: 11.1, 11.2, 11.3, 11.4_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in GitHub Actions and CI/CD pipelines | Task: Configure CI/CD quality gates following requirements 11.1-11.4. Create .github/workflows/ci.yml (main branch CI): trigger on push to main/master, jobs: build (runs on ubuntu-latest and windows-latest), steps: checkout, setup .NET 8, restore, build --no-restore, test --no-build, coverage collection, metrics check, format verification. Create .github/workflows/pr-validation.yml (PR validation): trigger on pull_request, same jobs as ci.yml plus: upload coverage report as artifact, add comment to PR with coverage summary if coverage < 80%. Both workflows should: use matrix strategy for multi-platform build (Linux, Windows), cache NuGet packages for speed, fail if any quality gate fails (build warnings, test failures, coverage < 80%, metrics violations, format issues). Configure branch protection: require pr-validation to pass before merge, require at least one approval, do not allow bypassing. Install reportgenerator tool for coverage reports: 'dotnet tool install dotnet-reportgenerator-globaltool'. Generate HTML coverage report and upload as artifact. | Restrictions: Do not deploy or publish from these workflows (only validation). Use same thresholds as pre-commit hooks (80% coverage, 500/50/15 metrics). Ensure workflows complete in <10 minutes. Cache dependencies for speed. Do not run on [skip ci] commits. Use latest stable .NET 8 SDK. | Leverage: Reference existing .husky/task-runner.json for task commands. Use same scripts/check-coverage.sh for coverage validation. Use same tools/MetricsChecker for metrics. Reference design document section "CI/CD Quality Gates" for pipeline structure. Use dotnet test --collect:"XPlat Code Coverage" same as pre-commit. Generate coverage report with reportgenerator for artifacts. | Requirements: Requirement 11.1 (automated PR builds with all checks), Requirement 11.2 (quality gate failures block merge), Requirement 11.3 (coverage reports as artifacts), Requirement 11.4 (coverage diff on PRs). | Success: ci.yml and pr-validation.yml created, workflows run on push/PR, all quality checks enforced in CI, coverage reports uploaded as artifacts, PR comments show coverage summary, branch protection configured (if applicable), workflows complete in <10 minutes, build succeeds on both Linux and Windows. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 5: Advanced Testing

- [ ] 17. Create end-to-end tests with Avalonia headless
  - Files:
    - TeaLauncher.Avalonia.Tests/EndToEnd/ApplicationLifecycleTests.cs
    - TeaLauncher.Avalonia.Tests/EndToEnd/UserWorkflowTests.cs
    - TeaLauncher.Avalonia.Tests/EndToEnd/SpecialCommandsTests.cs
    - TeaLauncher.Avalonia.Tests/EndToEnd/Fixtures/e2e-test-config.yaml
  - Write end-to-end tests simulating complete application workflows
  - Purpose: Validate critical user journeys from UI to command execution
  - _Leverage: Avalonia.Headless NuGet package, all implemented services_
  - _Requirements: 9.1, 9.2, 9.3, 9.4_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: E2E Test Engineer with expertise in Avalonia UI testing and headless testing | Task: Create end-to-end tests following requirements 9.1-9.4. Add NuGet package Avalonia.Headless (version 11.2.2 matching Avalonia version) to test project. Add NuGet package Avalonia.Headless.NUnit for NUnit integration. Create e2e-test-config.yaml fixture with test commands. ApplicationLifecycleTests: test CompleteLifecycle_StartInitializeExit (start app with headless, initialize config, verify app state, exit cleanly), test InitializationFailure_InvalidConfig (start with invalid config, verify error handling). UserWorkflowTests: test LaunchApp_ExecuteCommand_VerifyExecution (initialize app → simulate hotkey → verify command executed - mock Process.Start to verify call), test AutoCompletion_TabKey_CompletesCommand (type partial command → press Tab → verify completion), test CommandInput_EnterKey_ExecutesAndHides (type command → press Enter → verify window hidden). SpecialCommandsTests: test ReloadCommand_UpdatesCommands (execute !reload → verify new commands loaded), test VersionCommand_ShowsVersion (execute !version → verify version info), test ExitCommand_ClosesApp (execute !exit → verify app exits). Use Avalonia.Headless APIs: AvaloniaApp.BuildAvaloniaApp().UseHeadless().StartWithMainWindow<MainWindow>(). Mock platform-specific services (IHotkeyManager, IIMEController) for Linux testing. Use headless keyboard input simulation. Verify state through public properties and method calls, not visual rendering. | Restrictions: Tests must run on Linux without Windows display server. Do not test actual window rendering or visual appearance. Focus on workflow logic not UI pixels. Mock Process.Start to avoid actually launching applications during tests. Tests should complete in <30 seconds total. Use temporary config files created per test. Clean up resources in [TearDown]. | Leverage: Reference Avalonia.Headless documentation for headless testing setup. Reference existing EndToEnd/ directory structure. Use TestServiceProvider to configure DI for headless app. Reference design document section "End-to-End Testing" for test scenarios and patterns. Mock platform services same as integration tests. | Requirements: Requirement 9.1 (e2e test scenarios covering critical workflows), Requirement 9.2 (headless Avalonia on Linux), Requirement 9.3 (EndToEnd/ directory), Requirement 9.4 (<30 second completion). | Success: Avalonia.Headless packages added, three e2e test classes created, e2e-test-config.yaml fixture created, tests simulate complete user workflows, headless testing works on Linux, platform services mocked, tests verify end-to-end behavior, Process.Start mocked to prevent actual execution, tests complete in <30 seconds, all tests pass. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 18. Create TESTING.md documentation
  - File: TESTING.md
  - Document testing strategy, patterns, and how to run tests
  - Purpose: Help contributors understand testing approach and write effective tests
  - _Leverage: All test files created in previous tasks_
  - _Requirements: 10.1, 10.2, 10.3, 10.4_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in testing documentation and best practices | Task: Create comprehensive TESTING.md documentation following requirements 10.1-10.4. Document structure: ## Overview (testing strategy and importance), ## Test Categories (Unit/Integration/E2E with when to use each), ## Running Tests (commands: 'dotnet test' for all, 'dotnet test --filter FullyQualifiedName~Unit' for unit only, with coverage: 'dotnet test --collect:"XPlat Code Coverage"', view coverage: 'dotnet tool run reportgenerator ...'), ## Testing Patterns (AAA pattern with code examples, test naming conventions [MethodName]_[Scenario]_[ExpectedResult], mocking with NSubstitute examples, FluentAssertions usage), ## Coverage Thresholds (80% overall, 90% Domain/Application), ## Writing Tests section (how to add new tests, test organization matching src structure, using TestServiceProvider and MockFactory, creating test fixtures), ## Pre-commit Verification (explain Husky.Net hooks, how to run checks manually, how to bypass with --no-verify if needed), ## CI/CD Integration (explain GitHub Actions workflows, coverage reports, PR validation), ## Troubleshooting (common test failures and solutions), ## Examples (link to example tests: CommandRegistryServiceTests, CommandWorkflowTests, UserWorkflowTests). Include code snippets showing: unit test example with NSubstitute mocks, integration test example with real DI container, e2e test example with Avalonia headless. Include commands for coverage reports and metrics checking. Document how to generate HTML coverage report locally. | Restrictions: Do not include outdated information. Keep documentation concise but complete. Use actual examples from the codebase (not pseudo-code). Ensure commands are copy-pasteable and work. Document both Linux and Windows considerations where different. Keep formatting consistent with existing README.md style. | Leverage: Reference all test files created in tasks 11, 12, 17. Reference design document section "Test Documentation and Strategy" for content structure. Reference existing test patterns in CommandRegistryServiceTests, CommandWorkflowTests. Check actual coverage commands work before documenting. | Requirements: Requirement 10.1 (TESTING.md documenting strategy and patterns), Requirement 10.2 (XML documentation on test classes), Requirement 10.3 (inline comments for complex scenarios), Requirement 10.4 (new developers can run tests and understand output). | Success: TESTING.md created in repository root, covers all test categories comprehensively, includes working commands for running tests and coverage, documents AAA pattern with examples, explains thresholds and quality gates, shows code examples from real tests, troubleshooting section helpful, easy for new developers to understand, follows markdown best practices. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 6: Validation

- [ ] 19. Run full test suite and verify coverage thresholds
  - Files: (validation task, modifies test files if needed to reach coverage)
  - Execute complete test suite and verify coverage meets requirements
  - Purpose: Ensure all tests pass and coverage thresholds are met
  - _Leverage: All tests from tasks 11, 12, 17_
  - _Requirements: 6.2, 7.4, 8.1_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Lead with expertise in test execution and coverage analysis | Task: Validate test suite following requirements 6.2, 7.4, and 8.1. Run complete test suite: 'dotnet test --collect:"XPlat Code Coverage" --results-directory ./coverage'. Generate HTML coverage report: 'dotnet tool install dotnet-reportgenerator-globaltool' then 'reportgenerator -reports:./coverage/**/coverage.cobertura.xml -targetdir:./coverage-report -reporttypes:Html'. Open coverage-report/index.html and verify: overall line coverage ≥80%, overall branch coverage ≥70%, Domain layer (TeaLauncher.Avalonia.Domain) coverage ≥90%, Application layer (TeaLauncher.Avalonia.Application) coverage ≥90%. If coverage insufficient: identify uncovered lines in report, add missing test cases to appropriate test files (unit tests for Application/Domain, integration tests for workflows), focus on edge cases and error paths. Verify test performance: unit tests complete in ≤5 seconds, integration tests complete in ≤15 seconds, e2e tests complete in ≤30 seconds. Run tests multiple times to ensure no flakiness (tests produce consistent results). Document coverage results: create coverage-summary.txt with coverage percentages by layer. | Restrictions: Do not lower coverage thresholds to meet requirements. Add tests to reach thresholds. Do not test trivial code (getters/setters, auto-properties). Focus on meaningful coverage (business logic, error handling, edge cases). If some platform-specific code cannot be tested on Linux, document as limitation. Ensure tests are deterministic (no timing dependencies, no external service calls). | Leverage: Use reportgenerator for detailed coverage visualization. Reference design document section "Test Coverage" requirements. Review uncovered lines in HTML report to identify missing tests. Reference existing test patterns to add new tests consistently. | Requirements: Requirement 6.2 (80% overall, 70% branch coverage), Requirement 7.4 (90% Domain/Application coverage), Requirement 8.1 (integration tests for critical flows). | Success: Full test suite runs successfully, all tests pass, coverage report generated, overall coverage ≥80% line / ≥70% branch, Domain layer ≥90%, Application layer ≥90%, test performance within limits (<5s unit, <15s integration, <30s e2e), no flaky tests, coverage-summary.txt created documenting results. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 20. Test pre-commit hooks and quality gates in real workflow
  - Files: (validation task, no new files)
  - Perform end-to-end validation of pre-commit hooks and CI/CD pipeline
  - Purpose: Verify quality gates work correctly in real development workflow
  - _Leverage: .husky/pre-commit, GitHub Actions workflows, all quality tools_
  - _Requirements: 5.1, 5.2, 5.3, 5.5, 11.1, 11.2_
  - _Prompt: Implement the task for spec architecture-testability-improvements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in quality automation and workflow validation | Task: Validate complete quality gate workflow following requirements 5.1-5.3, 5.5, and 11.1-11.2. Test pre-commit hooks locally: 1) Make valid code change (add comment), attempt commit → verify hooks run and commit succeeds in <30s. 2) Make invalid change (add method with complexity 20), attempt commit → verify MetricsChecker blocks commit with clear error message. 3) Add code without tests causing coverage to drop below 80%, attempt commit → verify coverage check blocks commit. 4) Use 'git commit --no-verify' → verify commit bypasses hooks (document this is only for emergencies). Test CI/CD pipeline: 1) Create test branch, push valid changes → verify ci.yml workflow passes all checks. 2) Create PR with invalid changes (metrics violation) → verify pr-validation.yml fails, merge is blocked. 3) Create PR with valid changes → verify workflow passes, coverage report uploaded, PR can be merged. 4) Verify build works on both Linux and Windows runners. Test metrics tool manually: run 'dotnet run --project tools/MetricsChecker/MetricsChecker.csproj -- TeaLauncher.Avalonia.csproj' → verify current code passes all thresholds. Intentionally violate each threshold to verify detection: create 600-line file → verify detected, create method with 60 lines → verify detected, create method with complexity 20 → verify detected. Document workflow in commit message or PR description. Clean up test branches after validation. | Restrictions: Do not commit invalid code to main branch. Use test branches for validation. Do not disable or weaken quality gates. Do not modify thresholds to make tests pass. Verify hooks complete within 30 second requirement. Ensure CI workflows complete within 10 minutes. | Leverage: Use existing .husky/pre-commit hooks. Use existing GitHub Actions workflows. Use existing MetricsChecker tool. Reference design document section "Pre-commit Verification System" for workflow. Test real scenarios that developers will encounter. | Requirements: Requirement 5.1 (pre-commit hooks run automatically), Requirement 5.2 (metrics thresholds enforced), Requirement 5.3 (clear error messages), Requirement 5.5 (30 second timeout), Requirement 11.1 (automated CI builds), Requirement 11.2 (quality gates block merge). | Success: Pre-commit hooks work correctly (block invalid commits, allow valid commits, complete in <30s, clear error messages on violations), CI/CD workflows work correctly (run on push/PR, enforce all quality gates, upload artifacts, block merge on failure, work on Linux and Windows), MetricsChecker correctly detects all threshold violations, coverage checking works correctly, format checking works, --no-verify bypass documented, real workflow validated end-to-end. Edit tasks.md to mark this task [-] when starting and [x] when complete._
