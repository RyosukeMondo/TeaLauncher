# Tasks Document

## Phase 1: Dialog Service Infrastructure

- [x] 1. Create IDialogService interface in Domain layer
  - Files:
    - TeaLauncher.Avalonia/Domain/Interfaces/IDialogService.cs
  - Create interface with three methods: ShowMessageAsync, ShowConfirmAsync, ShowErrorAsync
  - Add XML documentation for each method
  - Purpose: Define contract for UI dialog operations
  - _Leverage: Existing Domain/Interfaces patterns (ICommandRegistry, IAutoCompleter)_
  - _Requirements: 2.1, 2.2_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Interface Designer with expertise in clean architecture and UI abstractions | Task: Create IDialogService interface in Domain/Interfaces/ following requirements 2.1 and 2.2. Define three async methods: ShowMessageAsync(string title, string message) returns Task, ShowConfirmAsync(string title, string message) returns Task<bool>, ShowErrorAsync(string title, string message) returns Task. Add comprehensive XML documentation explaining each method's purpose, parameters, and return values. Follow existing interface patterns from ICommandRegistry and IAutoCompleter in same directory. | Restrictions: Do not add any implementation logic (interface only). Do not add dependencies outside System namespaces. Use Task for async operations, not async void. Keep interface focused (no utility methods). | Leverage: Reference ICommandRegistry.cs and IAutoCompleter.cs in same directory for XML documentation style and namespace conventions. Use modern C# interface syntax. | Requirements: Requirement 2.1 (Dialog service contract definition), Requirement 2.2 (IDialogService methods for message, confirm, error). | Success: IDialogService.cs created with all three methods, comprehensive XML docs, compiles without errors, follows Domain layer conventions (no external dependencies), proper namespace TeaLauncher.Avalonia.Domain.Interfaces. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 2. Implement AvaloniaDialogService for production use
  - Files:
    - TeaLauncher.Avalonia/Infrastructure/UI/AvaloniaDialogService.cs (new directory: UI/)
  - Implement IDialogService using Avalonia MessageBox
  - Handle async wrapping of MessageBox.Show (which is sync)
  - Purpose: Provide production dialog implementation using Avalonia framework
  - _Leverage: Avalonia.Controls.MessageBox, existing Infrastructure layer patterns_
  - _Requirements: 2.3_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Avalonia UI Developer with expertise in dialog systems and async patterns | Task: Create AvaloniaDialogService in Infrastructure/UI/ directory implementing IDialogService following requirement 2.3. ShowMessageAsync: Wrap MessageBox.Show() in Task.Run or use Dispatcher.InvokeAsync, display message with OK button. ShowConfirmAsync: Create MessageBox with Yes/No buttons, return true for Yes, false for No. ShowErrorAsync: Call MessageBox with error icon/styling. All methods must be truly async (not sync wrappers). Add XML documentation. Create Infrastructure/UI directory if not exists. Use Avalonia 11.2.2 API (MessageBox may need window parameter for owner). | Restrictions: Do not block UI thread (use async properly). Do not show dialogs without window owner (will fail in headless). Handle null window owner gracefully for headless mode (detect and skip showing). Must implement interface exactly as defined. | Leverage: Reference Avalonia.Controls.MessageBox documentation. Reference existing Infrastructure services (WindowsHotkeyService, YamlConfigLoaderService) for implementation patterns and XML docs. Use async/await throughout. | Requirements: Requirement 2.3 (Production implementation using Avalonia MessageBox). | Success: AvaloniaDialogService.cs created in Infrastructure/UI/, implements all three interface methods correctly, uses real Avalonia MessageBox, handles async properly, XML documented, compiles without errors, namespace TeaLauncher.Avalonia.Infrastructure.UI. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 3. Create MockDialogService for testing
  - Files:
    - TeaLauncher.Avalonia.Tests/Utilities/MockDialogService.cs
  - Implement IDialogService that records dialog calls
  - Provide verification methods for test assertions
  - Purpose: Enable E2E tests to verify dialog interactions without display server
  - _Leverage: NSubstitute patterns, existing test utilities (MockFactory)_
  - _Requirements: 1.1, 1.4_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Infrastructure Developer with expertise in test doubles and mocking | Task: Create MockDialogService in Tests/Utilities/ implementing IDialogService following requirements 1.1 and 1.4. Maintain private List<DialogCall> _dialogCalls to record all dialog invocations. DialogCall record should have: DateTime Timestamp, string DialogType ("Message"/"Confirm"/"Error"), string Title, string Message, bool? UserResponse. ShowMessageAsync/ShowErrorAsync: record call, return Task.CompletedTask. ShowConfirmAsync: record call, return pre-configured bool (default true, allow SetConfirmResponse(bool) to configure). Provide public methods: VerifyMessageShown(string title, string message) using FluentAssertions, VerifyConfirmShown(string title), GetDialogCalls() returning IReadOnlyList<DialogCall>, ClearDialogHistory(). Add XML documentation. | Restrictions: Do not use NSubstitute Substitute.For (this is a manual mock, not a mocking framework mock). Keep simple and focused. All methods must be synchronous operations returning Tasks (no actual async work). Follow AAA pattern in implementation design. | Leverage: Reference MockFactory.cs for test utility patterns. Use FluentAssertions for VerifyMessageShown (e.g., _dialogCalls.Should().Contain(call => call.Title == title && call.Message == message)). Create DialogCall as nested public record. | Requirements: Requirement 1.1 (E2E tests use mock instead of real dialogs), Requirement 1.4 (Test mode captures dialogs for verification). | Success: MockDialogService.cs created, implements IDialogService, records all dialog calls with timestamp and details, provides verification methods, compiles and follows test utility patterns, namespace TeaLauncher.Avalonia.Tests.Utilities. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 4. Register IDialogService in DI containers
  - Files:
    - TeaLauncher.Avalonia/ServiceConfiguration.cs (modify)
    - TeaLauncher.Avalonia.Tests/Utilities/TestServiceProvider.cs (modify)
  - Add IDialogService registration to production DI container (Transient lifetime)
  - Add MockDialogService to test DI container
  - Purpose: Enable dependency injection of dialog service throughout application
  - _Leverage: Existing ServiceConfiguration patterns_
  - _Requirements: 2.5_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in .NET dependency injection | Task: Register IDialogService in both production and test DI containers following requirement 2.5. In ServiceConfiguration.cs: Add services.AddTransient<IDialogService, AvaloniaDialogService>() in ConfigureServices method (Transient because each dialog interaction should be independent, no state to preserve). In TestServiceProvider.cs: Modify CreateWithMocks() to register MockDialogService, modify CreateWithRealServices() to use AvaloniaDialogService (but note it may fail in headless - document this). Add summary comments explaining lifetime choice. | Restrictions: Do not use Singleton (dialogs are stateless, Transient is correct). Do not modify other service registrations. Ensure proper using statements added for new namespaces. Follow existing registration patterns exactly. | Leverage: Reference existing service registrations in ServiceConfiguration.cs for pattern (see ICommandRegistry, IAutoCompleter registrations). Reference TestServiceProvider.CreateWithMocks() for test registration pattern. Use same lifetime for test mocks as production. | Requirements: Requirement 2.5 (DI container registration for dialog service). | Success: ServiceConfiguration.cs updated with IDialogService â†’ AvaloniaDialogService registration (Transient), TestServiceProvider.cs updated to provide MockDialogService in both CreateWithMocks and CreateWithRealServices methods, compiles without errors, services resolvable from DI container. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 2: MainWindow Refactoring

- [x] 5. Refactor MainWindow to inject and use IDialogService
  - File: TeaLauncher.Avalonia/Views/MainWindow.axaml.cs
  - Add IDialogService constructor parameter
  - Replace all MessageBox.Show calls with IDialogService methods
  - Purpose: Make MainWindow testable by removing direct UI dependencies
  - _Leverage: Existing MainWindow code, IDialogService interface_
  - _Requirements: 1.1, 2.5_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Avalonia Developer with expertise in MVVM and dependency injection | Task: Refactor MainWindow.axaml.cs to use IDialogService following requirements 1.1 and 2.5. Find MainWindow constructor, add IDialogService dialogService parameter. Store in private readonly field _dialogService. Find all MessageBox.Show() calls (search for "MessageBox" in file). Replace MessageBox.Show(title, message) with await _dialogService.ShowMessageAsync(title, message). Replace error MessageBox calls with await _dialogService.ShowErrorAsync(title, message). If any confirmation dialogs exist, replace with bool result = await _dialogService.ShowConfirmAsync(title, message). Ensure all calling methods are async (add async keyword, change return type to Task if needed). Update XML documentation on constructor. | Restrictions: Do not change MainWindow UI logic (only dialog calls). Do not remove existing error handling. Ensure all dialog calls are awaited. Do not make synchronous calls to async methods (no .Result or .Wait()). | Leverage: Read current MainWindow.axaml.cs to find all MessageBox usage locations. Reference IDialogService interface for correct method signatures. Use async/await best practices (await all dialog calls, propagate async through call chain). | Requirements: Requirement 1.1 (Remove direct MessageBox for testability), Requirement 2.5 (Inject IDialogService via DI). | Success: MainWindow constructor accepts IDialogService, all MessageBox.Show calls replaced with IDialogService calls, all methods properly async/await, compiles without errors, application still shows dialogs correctly when run. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 6. Update MockFactory to provide dialog service mocks
  - File: TeaLauncher.Avalonia.Tests/Utilities/MockFactory.cs
  - Add CreateMockDialogService() helper method
  - Purpose: Centralize mock dialog service creation for test consistency
  - _Leverage: Existing MockFactory patterns_
  - _Requirements: 1.1_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Utility Developer with expertise in test helper patterns | Task: Add CreateMockDialogService() static method to MockFactory.cs following requirement 1.1. Method signature: public static MockDialogService CreateMockDialogService(bool defaultConfirmResponse = true). Method should: Create new MockDialogService instance, call SetConfirmResponse(defaultConfirmResponse) if that method exists on MockDialogService, return the instance. Add XML documentation explaining the method and defaultConfirmResponse parameter. Follow existing MockFactory patterns (see CreateMockCommandRegistry, CreateMockAutoCompleter methods for style). | Restrictions: Do not modify existing MockFactory methods. Keep method simple and focused. Follow static factory pattern like other methods in file. | Leverage: Reference existing CreateMockCommandRegistry and CreateMockAutoCompleter methods in same file for implementation pattern and XML documentation style. Ensure consistent code style with rest of file. | Requirements: Requirement 1.1 (Centralized mock creation for tests). | Success: MockFactory.cs updated with CreateMockDialogService method, follows existing patterns, properly documented, compiles without errors, returns configured MockDialogService instance. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 7. Add smoke test to verify MainWindow with dialog service
  - File: TeaLauncher.Avalonia.Tests/Views/MainWindowTests.cs (modify existing or create new tests)
  - Create basic test that MainWindow can be constructed with MockDialogService
  - Verify dialog calls are captured
  - Purpose: Ensure MainWindow refactoring didn't break basic functionality
  - _Leverage: Existing MainWindowTests, MockDialogService_
  - _Requirements: 1.1_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in unit testing UI components | Task: Add smoke tests to MainWindowTests.cs (or create file if doesn't exist) following requirement 1.1. Create test: Constructor_WithDialogService_ShouldSucceed - Arrange: Create MockDialogService, Act: Construct MainWindow with test config path and dialog service, Assert: window.Should().NotBeNull(). Create test: ShowErrorDialog_CallsDialogService - Arrange: Create MockDialogService and MainWindow, Act: Trigger an error scenario in MainWindow that shows error dialog (may need to call internal method or simulate error condition), Assert: mockDialogService.GetDialogCalls().Should().ContainSingle() and verify dialog type is "Error". Add [TestFixture] attribute to class, use [SetUp] and [TearDown] for test isolation. | Restrictions: Do not test full MainWindow UI rendering (only dialog service integration). Keep tests simple and focused. Use AAA pattern. Do not duplicate existing MainWindow tests. | Leverage: Reference existing MainWindowTests.cs if exists. Use MockFactory.CreateMockDialogService() for mock creation. Use FluentAssertions for assertions. Reference UserWorkflowTests.cs in EndToEnd/ for MainWindow construction patterns. | Requirements: Requirement 1.1 (Verify dialog service integration works). | Success: MainWindowTests.cs has smoke tests for dialog service integration, tests verify MockDialogService is called, tests pass, follow AAA pattern and naming conventions. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 3: E2E Test Fixes

- [x] 8. Update TestServiceProvider to use MockDialogService by default
  - File: TeaLauncher.Avalonia.Tests/Utilities/TestServiceProvider.cs
  - Ensure MockDialogService is provided in all test scenarios
  - Purpose: Make all E2E tests use mockable dialog service
  - _Leverage: Existing TestServiceProvider, MockDialogService_
  - _Requirements: 1.1, 1.4_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Infrastructure Engineer with expertise in test service providers | Task: Update TestServiceProvider.cs to provide MockDialogService by default following requirements 1.1 and 1.4. Review CreateWithMocks() method - ensure IDialogService is registered as MockDialogService (should already be done in task 4, verify). Review CreateWithRealServices() method - CHANGE to use MockDialogService instead of AvaloniaDialogService because headless mode cannot show real dialogs. Add comment explaining why even "real services" uses MockDialogService for IDialogService (headless limitation). If CreateCustom method exists, ensure it defaults to MockDialogService unless explicitly overridden. | Restrictions: Do not break existing test service provider functionality. Do not use AvaloniaDialogService in any test scenario (will fail in headless). Ensure backward compatibility with existing tests. | Leverage: Reference task 4 changes to ServiceConfiguration and TestServiceProvider. Verify registration is correct. Add inline comments explaining headless mode limitation. | Requirements: Requirement 1.1 (E2E tests use mock dialogs), Requirement 1.4 (Test mode uses MockDialogService). | Success: TestServiceProvider.cs provides MockDialogService in all scenarios, comments explain headless limitation, all existing tests still compile and pass, no AvaloniaDialogService in test code. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 9. Fix failing E2E tests in ApplicationLifecycleTests.cs
  - File: TeaLauncher.Avalonia.Tests/EndToEnd/ApplicationLifecycleTests.cs
  - Update tests to work with MockDialogService
  - Remove assumptions about dialog rendering
  - Purpose: Fix 5 failing E2E tests in application lifecycle category
  - _Leverage: MockDialogService, TestServiceProvider_
  - _Requirements: 1.1, 1.3_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: E2E Test Engineer with expertise in Avalonia headless testing | Task: Fix failing E2E tests in ApplicationLifecycleTests.cs following requirements 1.1 and 1.3. Review each failing test (should be marked in coverage-summary.txt as failures). For CompleteLifecycle_StartInitializeExit_ShouldSucceed: Instead of failing on dialog show, verify that MainWindow initializes correctly and MockDialogService is available. If test expects error dialogs, verify via mockDialogService.GetDialogCalls() instead of catching dialog exceptions. For initialization failure tests: Verify error dialogs are shown via MockDialogService instead of expecting exceptions. Update test assertions to verify dialog content (title, message) using VerifyMessageShown or GetDialogCalls. Remove any try-catch blocks that were catching dialog exceptions. Ensure all 5 tests pass. | Restrictions: Do not change test intent (still test same scenarios). Do not skip tests or mark as [Ignore]. Maintain headless compatibility (no real dialogs). Follow AAA pattern in test structure. | Leverage: Use MockDialogService.GetDialogCalls() for verification. Reference MockDialogService.VerifyMessageShown for assertions. Read current failing test code to understand what they're testing. Maintain existing test names and structure. | Requirements: Requirement 1.1 (Use mock dialogs), Requirement 1.3 (All E2E tests pass). | Success: All tests in ApplicationLifecycleTests.cs pass, tests verify dialog interactions via MockDialogService, no dialog rendering exceptions, tests still validate application lifecycle correctly. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 10. Fix failing E2E tests in UserWorkflowTests.cs and SpecialCommandsTests.cs
  - Files:
    - TeaLauncher.Avalonia.Tests/EndToEnd/UserWorkflowTests.cs
    - TeaLauncher.Avalonia.Tests/EndToEnd/SpecialCommandsTests.cs
  - Update remaining 12 failing E2E tests with MockDialogService
  - Verify dialog interactions instead of rendering
  - Purpose: Fix all remaining E2E test failures (reach 17/17 passing)
  - _Leverage: MockDialogService, patterns from task 9_
  - _Requirements: 1.1, 1.3_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: E2E Test Engineer with expertise in user workflow testing | Task: Fix remaining 12 failing E2E tests in UserWorkflowTests.cs and SpecialCommandsTests.cs following requirements 1.1 and 1.3. Apply same patterns from task 9: Replace dialog exception handling with MockDialogService verification. UserWorkflowTests: TypeCommand_PressEnter_ShouldExecuteCommand - verify command execution, check for success/error dialogs via mock. InvalidCommand_DoesNotExecute - verify error dialog shown with correct message. AutoCompletion_TabKey_CompletesCommand - no dialog changes needed, ensure test setup uses MockDialogService. MultipleCommands_Sequential_ShouldWork - verify multiple operations, check dialog calls if errors occur. SpecialCommandsTests: ReloadCommand_UpdatesConfig_ShouldWork - verify reload success dialog. VersionCommand_ShowsVersion_InDialog - verify version info dialog shown with MockDialogService, check dialog message contains version. ExitCommand_ClosesApp - verify exit confirmation dialog if exists. Update all assertions to use FluentAssertions and MockDialogService API. | Restrictions: Do not change test scenarios or intents. Do not mark tests as [Ignore]. All tests must pass in headless mode. Maintain consistent style with task 9 fixes. | Leverage: Use patterns established in task 9 (ApplicationLifecycleTests fixes). Reference MockDialogService.VerifyMessageShown, GetDialogCalls, VerifyConfirmShown methods. Read failing test code to understand what each tests. Apply consistent fix pattern across all tests. | Requirements: Requirement 1.1 (Mock dialogs in E2E tests), Requirement 1.3 (All 17 E2E tests pass). | Success: All 12 tests in UserWorkflowTests.cs and SpecialCommandsTests.cs pass, total E2E test count is 17/17 passing, tests verify dialogs via MockDialogService, headless mode works correctly, tests validate user workflows accurately. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 4: Edge Cases & Performance

- [x] 11. Create EdgeCaseTestFixtures with test data
  - File: TeaLauncher.Avalonia.Tests/Utilities/EdgeCaseTestFixtures.cs
  - Provide unicode commands, special characters, large datasets
  - Purpose: Centralize edge case test data for reuse
  - _Leverage: Existing TestFixtures patterns_
  - _Requirements: 3.1, 3.2, 3.3_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Test Data Specialist with expertise in edge case design | Task: Create EdgeCaseTestFixtures.cs in Tests/Utilities/ following requirements 3.1-3.3. Create static class with public static properties: UnicodeCommandNames (List<string>) with 10+ unicode command names (Chinese, Japanese, Arabic, Emoji, mixed scripts); SpecialCharacterArguments (List<string>) with special chars (quotes: "arg with \"quotes\"", backslashes: "path\\to\\file", pipes: "arg|with|pipes", unicode: "Ñ„Ð°Ð¹Ð».txt"); LargeWordList (List<string>) with 1000 words for autocomplete stress testing (generate programmatically or use common word list); MalformedYamlSamples (Dictionary<string, string>) with key=description, value=YAML (missing colon, wrong indentation, invalid escape, duplicate key, etc.). Add XML documentation on each property explaining the edge cases covered. Follow TestFixtures.cs patterns. | Restrictions: Do not hardcode 1000 words manually (generate with loop or use algorithm). Keep fixture data realistic (actual edge cases that could occur). Ensure unicode strings are valid UTF-8. Make all properties static readonly. | Leverage: Reference TestFixtures.cs for static property patterns and XML documentation style. Use StringBuilder or LINQ for generating LargeWordList efficiently (e.g., word1, word2, ..., word1000). Include diverse unicode scripts in UnicodeCommandNames. | Requirements: Requirement 3.1 (Special character handling), Requirement 3.2 (Unicode support), Requirement 3.3 (Large dataset performance). | Success: EdgeCaseTestFixtures.cs created, all four properties defined with realistic edge case data, LargeWordList has 1000+ items, unicode data is diverse, YAML samples cover common syntax errors, XML documented, compiles without errors. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 12. Add edge case tests to existing service test files
  - Files:
    - TeaLauncher.Avalonia.Tests/Application/Services/CommandExecutorServiceTests.cs (add tests)
    - TeaLauncher.Avalonia.Tests/Application/Services/CommandRegistryServiceTests.cs (add tests)
    - TeaLauncher.Avalonia.Tests/Application/Services/AutoCompleterServiceTests.cs (add tests)
    - TeaLauncher.Avalonia.Tests/Infrastructure/Configuration/YamlConfigLoaderServiceTests.cs (add tests)
  - Add 15+ edge case tests across service files
  - Purpose: Ensure services handle uncommon scenarios correctly
  - _Leverage: EdgeCaseTestFixtures, existing test patterns_
  - _Requirements: 3.1, 3.2, 3.3, 3.5_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer specializing in edge case testing | Task: Add edge case tests to existing service test files following requirements 3.1-3.5. CommandExecutorServiceTests.cs: Add ExecuteAsync_WithQuotedArguments_ParsesCorrectly (use EdgeCaseTestFixtures.SpecialCharacterArguments), ExecuteAsync_WithBackslashPaths_HandlesCorrectly, ExecuteAsync_WithUnicodeInArguments_ExecutesCorrectly, ExecuteAsync_ConcurrentCalls_MaintainsState (test thread safety). CommandRegistryServiceTests.cs: Add RegisterCommand_WithUnicodeNames_StoresCorrectly (use UnicodeCommandNames), HasCommand_WithUnicodeLookup_FindsCorrectly (case-insensitive unicode), GetAllCommands_EmptyRegistry_ReturnsEmpty. AutoCompleterServiceTests.cs: Add GetCandidates_UnicodeWords_ReturnsCorrectly, AutoComplete_EmptyWordList_ReturnsEmptyString, UpdateWordList_WithNull_HandlesGracefully. YamlConfigLoaderServiceTests.cs: Add LoadConfiguration_MalformedYaml_ProvidesLineNumber (iterate through MalformedYamlSamples), LoadConfiguration_UnicodeCommands_LoadsCorrectly. Use AAA pattern, descriptive test names, FluentAssertions. Add XML summary to new test methods. | Restrictions: Do not modify existing passing tests. Add tests as new methods in existing test files. Maintain test isolation (use [SetUp] properly). Each test should test one edge case. Use EdgeCaseTestFixtures for test data. | Leverage: Use EdgeCaseTestFixtures.UnicodeCommandNames, SpecialCharacterArguments, MalformedYamlSamples. Reference existing test methods in same files for mocking patterns and assertion style. Use FluentAssertions .Should() syntax. Create temporary YAML files in tests using Path.GetTempPath(). | Requirements: Requirement 3.1 (Special characters), Requirement 3.2 (Unicode), Requirement 3.3 (Large datasets), Requirement 3.5 (Error messages). | Success: 15+ new edge case tests added across 4 test files, all tests pass, tests use EdgeCaseTestFixtures, tests follow AAA pattern and naming conventions, XML documented, code coverage increases for edge case scenarios. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [x] 13. Create PerformanceTestBase and performance test infrastructure
  - Files:
    - TeaLauncher.Avalonia.Tests/Performance/PerformanceTestBase.cs (new directory: Performance/)
    - TeaLauncher.Avalonia.Tests/Performance/PerformanceResult.cs
  - Create base class for performance tests with timing utilities
  - Purpose: Provide reusable infrastructure for performance testing
  - _Leverage: TestBase.cs patterns, Stopwatch_
  - _Requirements: 4.1, 4.2, 4.3, 4.4, 4.5_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Performance Test Engineer with expertise in .NET performance measurement | Task: Create performance test infrastructure in new Tests/Performance/ directory following requirements 4.1-4.5. Create PerformanceResult.cs: Public record with properties OperationName (string), Duration (TimeSpan), MaxAllowedMs (int), Passed (bool). Add computed properties: DurationMs (double, Duration.TotalMilliseconds), PercentageOfLimit (double, (DurationMs / MaxAllowedMs) * 100). Create PerformanceTestBase.cs: Abstract class with [TestFixture] attribute. Provide protected methods: TimeOperation(Action action) returns PerformanceResult - use Stopwatch to measure action execution, AssertDuration(PerformanceResult result) - use FluentAssertions result.Passed.Should().BeTrue() with custom message showing actual vs max. TimeOperationAsync(Func<Task> action) returns Task<PerformanceResult> - async version. Provide protected helper: CreatePerformanceResult(string name, TimeSpan duration, int maxMs). Add XML documentation explaining performance testing approach. | Restrictions: Do not include actual performance tests in this task (next task). Keep infrastructure generic and reusable. Use high-resolution Stopwatch (Stopwatch.StartNew/Stop). Ensure accurate timing (warm up JIT before critical measurements if needed, document this). | Leverage: Reference TestBase.cs for base class patterns and SetUp/TearDown. Use System.Diagnostics.Stopwatch for timing. Use FluentAssertions for assertions with custom messages (WithMessage). Create PerformanceResult as simple record for data passing. | Requirements: Requirements 4.1-4.5 (Performance measurement infrastructure for 100ms/50ms/200ms thresholds). | Success: Performance/ directory created, PerformanceTestBase.cs and PerformanceResult.cs created, timing utilities working correctly, XML documented, ready for performance test implementation, compiles without errors. Edit tasks.md to mark this task [-] when starting and [x] when complete._

- [ ] 14. Implement performance tests for critical operations
  - Files:
    - TeaLauncher.Avalonia.Tests/Performance/CommandExecutionPerformanceTests.cs
    - TeaLauncher.Avalonia.Tests/Performance/AutoCompletePerformanceTests.cs
    - TeaLauncher.Avalonia.Tests/Performance/ConfigLoadPerformanceTests.cs
  - Create performance tests validating sub-100ms claims
  - Purpose: Validate TeaLauncher's performance guarantees with automated tests
  - _Leverage: PerformanceTestBase, EdgeCaseTestFixtures.LargeWordList_
  - _Requirements: 4.1, 4.2, 4.3, 4.4_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Performance Engineer with expertise in benchmarking and optimization | Task: Create three performance test files inheriting from PerformanceTestBase following requirements 4.1-4.4. CommandExecutionPerformanceTests.cs: Test CommandExecution_FromInputToStart_Within100ms - measure time from executor.ExecuteAsync() call to method return (should start process quickly, not wait for completion), max 100ms. Test ConcurrentCommands_NoPerformanceDegradation - execute 10 commands concurrently, verify average â‰¤ 100ms. AutoCompletePerformanceTests.cs: Test AutoComplete_With1000Words_Within50ms - use EdgeCaseTestFixtures.LargeWordList, measure AutoCompleteWord and GetCandidates, max 50ms. Test AutoComplete_WithUnicode_NoSlowdown - use UnicodeCommandNames, verify performance same as ASCII (max 50ms). ConfigLoadPerformanceTests.cs: Test ConfigLoad_100Commands_Within200ms - generate temp YAML with 100 commands, measure LoadConfiguration, max 200ms. Use [SetUp] to create test services, [TearDown] to cleanup. Use TimeOperationAsync for async operations. Call AssertDuration for all tests. Add XML docs explaining performance expectations. | Restrictions: Do not test UI rendering performance (headless limitation). Do not include network calls (mock external dependencies). Performance tests should be deterministic (run multiple iterations, take average if flaky). Document any known performance variability in comments. Use Release build for accurate measurements (check configuration). | Leverage: Inherit from PerformanceTestBase, use TimeOperationAsync. Use EdgeCaseTestFixtures.LargeWordList for autocomplete stress testing. Use TestServiceProvider for service creation. Create temporary YAML files for config load tests. Mock Process.Start to avoid actual process execution overhead. | Requirements: Requirement 4.1 (hotkey 100ms - not testable in headless), Requirement 4.2 (command execution 100ms), Requirement 4.3 (autocomplete 50ms), Requirement 4.4 (config load 200ms). | Success: Three performance test files created, 5+ performance tests implemented, tests validate performance requirements (100ms execution, 50ms autocomplete, 200ms config), all tests pass, tests use PerformanceTestBase utilities, XML documented. Edit tasks.md to mark this task [-] when starting and [x] when complete._

## Phase 5: Documentation

- [ ] 15. Update TESTING.md with new patterns and examples
  - File: TESTING.md
  - Add sections for dialog testing, performance testing, edge cases
  - Update examples to include new patterns
  - Purpose: Help developers understand and maintain test enhancements
  - _Leverage: Existing TESTING.md structure_
  - _Requirements: 5.1, 5.2, 5.3, 5.4_
  - _Prompt: Implement the task for spec test-enhancements, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in testing documentation | Task: Update TESTING.md following requirements 5.1-5.4. Add new sections: "Testing Dialogs in E2E Tests" (explain IDialogService abstraction, show MockDialogService usage example with code snippet, explain why AvaloniaDialogService doesn't work in headless). "Performance Testing Guidelines" (explain PerformanceTestBase, show example performance test with TimeOperationAsync, document thresholds: 100ms execution, 50ms autocomplete, 200ms config). "Edge Case Test Patterns" (explain EdgeCaseTestFixtures, show examples of unicode testing, special character testing, large dataset testing). Update "Running Tests" section to mention Performance/ directory tests. Update "Test Categories" section to add Performance as fourth category. Update "Writing Tests" section with dialog service and performance test examples. Update test count from 212 to 229+ tests. Add code examples from actual test files created in previous tasks. | Restrictions: Do not remove existing documentation. Maintain consistent markdown formatting. Keep explanations concise but complete. Use actual code examples, not pseudocode. Ensure commands are copy-pasteable. | Leverage: Reference existing TESTING.md structure and writing style. Copy actual code snippets from MockDialogService, PerformanceTestBase, and test files. Follow existing section organization (## Overview, ## Test Categories, etc.). Use markdown code blocks with csharp syntax highlighting. | Requirements: Requirement 5.1 (Clear error messages from tests already exists, verify), Requirement 5.2 (AAA pattern already documented, maintain), Requirement 5.3 (New patterns and examples added), Requirement 5.4 (Centralized utilities documented). | Success: TESTING.md updated with three new sections, code examples added, test count updated, performance testing documented, dialog testing documented, edge case patterns documented, markdown properly formatted, easy for developers to follow. Edit tasks.md to mark this task [-] when starting and [x] when complete._
